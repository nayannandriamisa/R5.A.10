{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compte rendu R5.A.10, ANDRIAMISA Nayann, Groupe Nayru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce fichier, nous verrons un compte rendu de la ressource du BUT informatique (R5.A.10) portant sur les nouvelles bases de données.    \n",
    "Elle est découpée en 3 grandes parties qui sont respectivement, sur la dénormalisation d'un schéma relationnel de tables et puis d'études des Systèmes de Gestion de Base de Données (SGBD) **Redis** et **MongoDB**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire :\n",
    "\n",
    "<ol>\n",
    "    <li><strong>Dénormalisation</strong></li>\n",
    "    <ol>\n",
    "        <li>Dénormalisation d'un schéma relationnel </li>\n",
    "        <li>Application de la représentation JSON</li>\n",
    "    </ol>\n",
    "    <li><strong>Redis</strong></li>\n",
    "    <ol>\n",
    "        <li>Bloom filters </li>\n",
    "        <li>SET_PY</li>\n",
    "        <li>PUB/SUB</li>\n",
    "    </ol>\n",
    "    <li><strong>MongoDB</strong></li>\n",
    "    <ol>\n",
    "        <li>Premiers pas avec MongoDB</li>\n",
    "        <li>Performances avec indexe</li>\n",
    "        <li>Performances sans indexe</li>\n",
    "    </ol>\n",
    "    <li><strong>Conclusion</strong></li>\n",
    "</ol>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dénormalisation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit de faire une sorte de fusion/factorisation d'un ensemble de tables SQL en une seule pour avoir une \"structure à plat\" afin de minimiser les jointures nécessaires aux opérations.    \n",
    "Le problème qui se pose est donc de savoir comment passer d'un schéma relationnel en SQL à un modèle NoSQL qui pourrait être implémenté avec nos gestionnaires.    \n",
    "<br>\n",
    "Il y a certains critères à respecter dans la fusion dans le cas d'une dénormalisation : \n",
    "- Les données qui sont fréquemment intérrogées doivent être rassemblées\n",
    "- Toutes les données d'un entité doivent être indépendantes\n",
    "- Une association avec des relations 1+n des deux côtés\n",
    "- Le taux de mise à jour est le même\n",
    "\n",
    "Une fois un schéma relationnel dénormalisé, pour être utilisé il convient de l'obetnir sous une représentation JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Exemple/exercice :</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dénormalisation d'un schéma relationnel SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un schéma relationnel en SQL, qui a déjà été étudié par les étudiants de 1ère année de BUT informatique pour la ressource ***Introduction aux bases de données et SQL*** :\n",
    "<br><br>\n",
    "<code>\n",
    "**AVIONS** (*NumAv, NomAv, CapAv, VilleAv*);<br>\n",
    "**PILOTES** (*NumPil, NomPil, NaisPil, VillePil*);<br>\n",
    "**CLIENTS**(*NumCl, NomCl, NumRueCl, NomRueCl, CodePosteCl, VilleCl*);<br>\n",
    "**VOLS**(*NumVol, VilleD, VilleA, DateD, HD, DateA, HA, NumPil, NumAv*);<br>\n",
    "**DEFCLASSES**(*NumVol, Classe, CoeffPrix*);<br>\n",
    "**RESERVATIONS** (*NumCl, NumVol, Classe, NbPlaces*);<br>\n",
    "</code>\n",
    "<br>\n",
    "Et afin de procéder à une dénormalisation de ce schéma relationnel, nous avons fusionné les tables **VOLS**, **DEFCLASSES** et **RESERVATIONS** par rapport à l'attribut \"*NumVol*\".    \n",
    "Voici le résultat obtenu après dénormalisation :\n",
    "\n",
    "<code>\n",
    "**AVIONS** (*NumAv, NomAv, CapAv, VilleAv*);<br>\n",
    "**PILOTES** (*NumPil, NomPil, NaisPil, VillePil*);<br>\n",
    "**CLIENTS**(*NumCl, NomCl, NumRueCl, NomRueCl, CodePosteCl, VilleCl*);<br>\n",
    "**VOLS**(*NumVol, VilleD, VilleA, DateD, HD, DateA, HA, NumPil, NumAv, Classe, CoeffPrix, NumCl, NbPlaces*)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Application de la représentation JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc commencer par mettre les fichiers dans le bon format, et devoir passer d'un document .txt à un document JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importations et installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On va commencer par les convertir en fichier .csv\n",
    "En utilisant la fonction <code>to_csv()</code> de la libraire Python : **Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avions = pd.read_csv('bddPilotes/AVIONS.txt', sep='\\t')\n",
    "avions.columns = [\"NumAv\", \"NomAv\", \"CapAv\", \"VilleAv\"]\n",
    "avions.to_csv('bddPilotes/CSV/AVIONS.csv', index=None)\n",
    "\n",
    "pilotes = pd.read_csv('bddPilotes/PILOTES.txt', sep='\\t')\n",
    "pilotes.columns = [\"NumPil\",\"NomPil\",\"NaisPil\",\"VillePil\"]\n",
    "pilotes.to_csv('bddPilotes/CSV/PILOTES.csv', index=None)\n",
    "\n",
    "clients = pd.read_csv('bddPilotes/CLIENTS.txt', sep='\\t')\n",
    "clients.columns = [\"NumCl\", \"NomCl\", \"NumRueCl\", \"NomRueCl\", \"CodePosteCl\", \"VilleCl\"]\n",
    "clients.to_csv('bddPilotes/CSV/CLIENTS.csv', index=None)\n",
    "\n",
    "vols = pd.read_csv('bddPilotes/VOLS.txt', sep='\\t')\n",
    "vols.columns = [\"NumVol\",\"VilleD\",\"VilleA\", \"DateD\",\"HD\",\"DateA\",\"HA\", \"NumPil\", \"NumAv\"]\n",
    "vols.to_csv('bddPilotes/CSV/VOLS.csv', index=None)\n",
    "\n",
    "defclasses = pd.read_csv('bddPilotes/DEFCLASSES.txt', sep='\\t')\n",
    "defclasses.columns = [\"NumVol\",\"Classe\", \"CoeffPrix\"]\n",
    "defclasses.to_csv('bddPilotes/CSV/DEFCLASSES.csv', index=None)\n",
    "\n",
    "reservations = pd.read_csv('bddPilotes/RESERVATIONS.txt', sep='\\t')\n",
    "reservations.columns = [\"NumCl\", \"NumVol\" ,\"Classe\", \"NbPlaces\"]\n",
    "reservations.to_csv('bddPilotes/CSV/RESERVATIONS.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis nous allons effectuer la jointure sur les classes **VOLS**, **DEFCLASSES** et **RESERVATIONS** en utilisant la fonction <code>merge()</code> pour l'étape de la dénormalisation.    \n",
    "Et nous finirons par convertir ces fichiers .csv au format JSON grâce à la fonction <code>to_json()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avions = pd.read_csv('bddPilotes/CSV/AVIONS.csv')\n",
    "pilotes = pd.read_csv('bddPilotes/CSV/PILOTES.csv')\n",
    "clients = pd.read_csv('bddPilotes/CSV/CLIENTS.csv')\n",
    "vols = pd.read_csv('bddPilotes/CSV/VOLS.csv')\n",
    "defclasses = pd.read_csv('bddPilotes/CSV/DEFCLASSES.csv')\n",
    "reservations = pd.read_csv('bddPilotes/CSV/RESERVATIONS.csv')\n",
    "\n",
    "fusion_vols_defclasses = pd.merge(vols, defclasses, how='inner', on=[\"NumVol\"])\n",
    "fusion_vols_defclasses_reservations = pd.merge(fusion_vols_defclasses, reservations, how='inner', on=[\"NumVol\"])\n",
    "fusion_vols_defclasses_reservations.to_csv('bddPilotes/CSV/VolsClassesReservations.csv', index=None)\n",
    "\n",
    "#print(fusion_vols_defclasses_reservations.head(50))\n",
    "\n",
    "avions.to_json('bddPilotes/JSON/AVIONS.json', orient=\"records\")\n",
    "pilotes.to_json('bddPilotes/JSON/PILOTES.json', orient=\"records\")\n",
    "clients.to_json('bddPilotes/JSON/CLIENTS.json', orient=\"records\")\n",
    "vols.to_json('bddPilotes/JSON/VOLS.json', orient=\"records\")\n",
    "defclasses.to_json('bddPilotes/JSON/DEFCLASSES.json', orient=\"records\")\n",
    "reservations.to_json('bddPilotes/JSON/RESERVATIONS.json', orient=\"records\")\n",
    "\n",
    "fusion = pd.read_csv('bddPilotes/CSV/VolsClassesReservations.csv')\n",
    "fusion.to_json('bddPilotes/JSON/VolsClassesReservations.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bddPilotes/JSON/AVIONS.json', 'r') as file:\n",
    "    data = pd.read_json(file)\n",
    "    print(data.head())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "with open('bddPilotes/JSON/PILOTES.json', 'r') as file:\n",
    "    data = pd.read_json(file)\n",
    "    print(data.head())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "with open('bddPilotes/JSON/VolsClassesReservations.json', 'r') as file:\n",
    "    data = pd.read_json(file)\n",
    "    print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fonction de jointure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_json(csv_file):\n",
    "\n",
    "    data_dict = {}\n",
    "    my_dict = {}\n",
    "    with open(csv_file, encoding = 'utf8') as csvfile:\n",
    "        my_reader = csv.DictReader(csvfile)\n",
    "        print(my_reader.fieldnames)\n",
    "        my_data = [my_row for my_row in my_reader]\n",
    "        for my_row in my_data:\n",
    "            #print(my_row)\n",
    "            my_dict = {}\n",
    "            my_dict[my_reader.fieldnames[0]] = my_row[my_reader.fieldnames[0]]\n",
    "            my_dict[my_reader.fieldnames[1]] = my_row[my_reader.fieldnames[1]]\n",
    "            data_dict[my_row[my_reader.fieldnames[2]]] = my_dict\n",
    "    print(\"====================\")\n",
    "    my_my_dict = {}\n",
    "    my_my_dict['test'] = data_dict\n",
    "    print(my_my_dict)\n",
    "    #for item in data_dict.items():\n",
    "    #    print(item)\n",
    "    #\n",
    "    # convert both intermediary results to JSON object\n",
    "    #\n",
    "    y = json.dumps(my_my_dict)\n",
    "    print(\"====================\")\n",
    "    print(y)\n",
    "    print(\"====================\")\n",
    "\n",
    "    return y\n",
    "\n",
    "def jointure(json1, json2):\n",
    "\n",
    "    from json import loads\n",
    "    from json import dumps\n",
    "\n",
    "    # First, transform json objects to dictionaries\n",
    "\n",
    "    d1_name = list(loads(json1))[0]\n",
    "    #print(d1_name)\n",
    "    d2_name = list(loads(json2))[0]\n",
    "    #print(d2_name)\n",
    "\n",
    "    d1 = loads(json1)[d1_name]\n",
    "    d2 = loads(json2)[d2_name]\n",
    "\n",
    "    #print(att_name,type(att_name))\n",
    "    # Second, iterate through dictionaries\n",
    "    d_res = {}\n",
    "    for key1, val1 in d1.items():\n",
    "        #print(key1, '==', val1)\n",
    "        for key2, val2 in d2.items():\n",
    "            #print(key1, '==', key2)\n",
    "            #print([ord(c) for c in key1],key1,[ord(c) for c in att_name],att_name)\n",
    "            if key1 == key2:\n",
    "                d = {}\n",
    "                d.update(val1)\n",
    "                d.update(val2)\n",
    "                #print(d)\n",
    "                d_res[key1] = d\n",
    "    my_my_dict = {}\n",
    "    my_my_dict['test'] = d_res\n",
    "    z = dumps(my_my_dict)\n",
    "\n",
    "    return z\n",
    "\n",
    "# Main program\n",
    " \n",
    "json_one = csv_to_json(\"bddPilotes/CSV/AVIONS.csv\")\n",
    "json_two = csv_to_json(\"bddPilotes/CSV/CLIENTS.csv\")\n",
    "\n",
    "d = jointure (json_one, json_two)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redis : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis est un SGBD et aussi un gestionnaire **NoSQL** *(Not Only SQL)* qui permet d'adopter le modèle \"clé-valeur\", ou aussi **JSON**, pour sa structure de données.    \n",
    "<br>\n",
    "Afin de se connecter à un serveur Redis depuis ce fichier Jupyter Notebook, pour pouvoir expérimenter ce gestionnaire, j'ai fait appel à ***Docker*** en lancant mon serveur depuis un conteneur avec la commande <code>redis-cli</code>    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Connexion à Redis</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Les installations et importations nécessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import csv\n",
    "from json import dumps\n",
    "import time\n",
    "\n",
    "client = redis.Redis(host='localhost', port=6380, decode_responses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test de la connection au serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout de données\n",
    "client.set(\"Nom\", \"Andriamisa\")\n",
    "client.set(\"Prenom\", \"Nayann\")\n",
    "\n",
    "# Récupération des données\n",
    "print(client.get(\"Nom\") + \" \" + client.get(\"Prenom\"))\n",
    "client.get(\"Ressource\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Les Bloom filters sur Redis</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le filtre de Bloom est une structure de données probabiliste utilisée pour tester l'appartenance d'un élément à un ensemble. Conçu pour être utilisé dans des situations où la mémoire est limitée et où des faux positifs sont tolérables, le filtre de Bloom permet d'économiser de l'espace en évitant le stockage direct des éléments de l'ensemble. Il utilise plusieurs fonctions de hachage pour attribuer à chaque élément plusieurs positions dans un tableau de bits. Lorsqu'on interroge le filtre pour la présence d'un élément, il renvoie probablement vrai (avec une certaine probabilité d'erreur), indiquant ainsi que l'élément pourrait être dans l'ensemble, ou certainement faux s'il n'est pas présent. Redis offre une implémentation efficace du filtre de Bloom, permettant son utilisation dans un contexte de base de données à mémoire vive avec des avantages significatifs en termes d'optimisation de l'utilisation de la mémoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Le code python implémentant les Blooms filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_bloom(csv_file, n) :\n",
    "\n",
    "    client.delete(\"bloom\")\n",
    "    client.execute_command(\"BF.RESERVE\", \"bloom\", \"0.01\", \"1700000\")\n",
    "\n",
    "    with open(csv_file, encoding='utf-8') as csvfile:\n",
    "        my_reader = csv.DictReader(csvfile, delimiter=',')\n",
    "        my_data = [my_row for my_row in my_reader]\n",
    "\n",
    "        pres = dup = 0\n",
    "        print('Création du filtre bloom sur', n, 'entrées')\n",
    "        st = time.process_time()\n",
    "\n",
    "        for my_row in my_data[0:n]:\n",
    "            if not client.execute_command(\"BF.EXISTS\", \"91630\", my_row['codecommune']):\n",
    "                client.execute_command(\"BF.ADD\", \"91630\", my_row['codecommune'])\n",
    "                pres = pres + 1\n",
    "            else:\n",
    "                dup = dup + 1\n",
    "\n",
    "        et = time.process_time()\n",
    "        res = et - st\n",
    "        print('Temps d\\'exécution CPU :', res, 'secondes')\n",
    "        print('Trouvé', dup, 'doublons dans l\\'entrée')\n",
    "\n",
    "# Step 1\n",
    "perf_bloom(\"pres2022comm.csv\", 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Set.py pour des test de performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici une fonction qui teste les performances d'ajout d'éléments distincts du fichier CSV à un ensemble Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_set(csv_file, n):\n",
    "\n",
    "    r = redis.Redis(host='localhost', port=6380, decode_responses=True)\n",
    "\n",
    "    r.delete(\"my_set\")\n",
    "\n",
    "    with open(csv_file, encoding = 'utf-8') as csvfile:\n",
    "        my_reader = csv.DictReader(csvfile,delimiter=',')\n",
    "        my_data = [my_row for my_row in my_reader]\n",
    "        #print(my_data)\n",
    "        pres = 0\n",
    "        dup = 0\n",
    "        print('Début du test de performance sur', n, 'entrées')\n",
    "        # get the start time\n",
    "        st = time.process_time()\n",
    "        for my_row in my_data[0:n]:\n",
    "            #print(my_row['M'])\n",
    "            if not r.sismember(\"my_set\", my_row['codecommune']):\n",
    "                r.sadd(\"my_set\", my_row['codecommune'])\n",
    "                pres += 1\n",
    "            else:\n",
    "                dup += 1\n",
    "        # get the end time\n",
    "        et = time.process_time()\n",
    "        # get execution time\n",
    "        res = et - st\n",
    "        print('Temps d\\'exécution CPU :', res, 'secondes')\n",
    "        print('Trouvé', dup, 'doublons dans l\\'entrée')\n",
    "#Step 1\n",
    " \n",
    "perf_set(\"pres2022comm.csv\", 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUB/SUB avec Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les installations et importations utilisées pour tester PUB/SUB avec Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import schedule\n",
    "\n",
    "client = redis.Redis(host=\"localhost\", port=6380, decode_responses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le code ci-dessous, la fonction <code>publish()</code> va publier un message qui sera un couple d'une **date** (à l'occurence la date du jour ainsi que l'heure) et d'une **CO2_value** (un nombre aléatoire entre 300 et 1000).    \n",
    "Ces messages seront publiés de manière espacée entre 1 et 3 secondes (aléatoirement aussi).  \n",
    "<br>  \n",
    "*Afin que la fonction PUB/SUB de Redis fonctionne correctemment, il faut éxécuter le publisher indépendamment du subscriber (dans un autre jupyter avec un noyau différent de celui utilisé dans ce compte rendu par exemple).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_co2_value():\n",
    "    return round(random.uniform(300, 1000))\n",
    "\n",
    "while True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    co2_value = generate_random_co2_value()\n",
    "    \n",
    "    # Créer un dictionnaire\n",
    "    data = {'date': current_date, 'co2_value': co2_value}\n",
    "    \n",
    "    # Convertir le dictionnaire en format JSON\n",
    "    json_data = json.dumps(data)\n",
    "    \n",
    "    client.publish('testPubSub', json_data)\n",
    "\n",
    "    #print(r.get('testPubSub'))\n",
    "\n",
    "    time.sleep(random.uniform(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subscriber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le code ci-dessous, la fonction <code>pubsub()</code> va définir le comportement du client et la fonction <code>psubscribe()</code> va dire que le client connecté est \"abonné\" aux messages publiés dans la chaîne \"testPubSub\".    \n",
    "Et elle executera toutes les minutes un calcul de la moyenne des valeurs de CO2 que le client a reçu dans la dernière minute.\n",
    "<br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    global my_list\n",
    "    #\n",
    "    # Calcul de la moyenne\n",
    "    #\n",
    "    if len(my_list) > 0:\n",
    "    \tprint(\"La moyenne des valeurs de CO2 sur la dernière minute est:\", sum(my_list) / len(my_list))\n",
    "    my_list.clear()\n",
    "\n",
    "schedule.every(10).seconds.do(func)\n",
    "\n",
    "p = client.pubsub()\n",
    "p.psubscribe('testPubSub')\n",
    "\n",
    "my_list = []\n",
    "\n",
    "first_message = True\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    m = p.get_message()\n",
    "    #schedule.run_pending()\n",
    "    if m != None:\n",
    "       if not first_message:\n",
    "          print('Message reçu:', m['data'])\n",
    "          data_str = m['data']\n",
    "          #data_str = m['data'].decode('utf-8')  # Décoder les bytes en str\n",
    "          data_dict = json.loads(data_str)  # Charger la chaîne JSON en un dictionnaire Python\n",
    "          co2_value = data_dict.get('co2_value')\n",
    "          if co2_value is not None:\n",
    "                my_list.append(co2_value)\n",
    "\n",
    "       else:\n",
    "        print(\"Erreur : ça marche pas\")\n",
    "        first_message = False\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB est un SGBD et aussi un gestionnaire **NoSQL** *(Not Only SQL)* qui permet d'adopter le modèle  **JSON** pour sa structure de données.  \n",
    "<br>\n",
    "Afin de se connecter à un serveur MongoDB depuis ce fichier Jupyter Notebook, pour pouvoir expérimenter ce gestionnaire, j'ai fait appel à ***Docker*** en lancant mon serveur depuis un conteneur avec la commande <code>mongosh</code>    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les installations et importations nécessaires pour l'expérimentation de MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import csv\n",
    "import time\n",
    "\n",
    "connection = pymongo.MongoClient(host=\"localhost\", port=27017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Premiers pas avec MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons voir dans la cellule suivante, différentes fonctions de la libraire Python **PyMongo** et qui nous permet d'intéragir avec la base de données MongoDB depuis ce fichier.\n",
    "On a utilisé les fonctions d'ajout <code>insert_one()</code> et <code>insert_many()</code>, les fonctions de suppression <code>delete_one()</code> et <code>delete_many()</code>, les fonctions de modifications <code>update_one()</code> et <code>update_many()</code> ainsi que la fonction de consultation <code>find()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = connection[\"db_R5A10\"]\n",
    "\n",
    "collection = db[\"test_bd_jupyter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout\n",
    "data = {\"Nom\":\"Andriamisa\", \"Prenom\":\"Nayann\"}\n",
    "collection.insert_one(data)\n",
    "data1 = [\n",
    "    {\"Date\":\"22-12-2023\", \"Jour\":\"Vendredi\", \"Mois\":\"Decembre\"},\n",
    "    {\"Date\":\"23-12-2023\", \"Jour\":\"Samedi\", \"Mois\":\"Decembre\"},\n",
    "    {\"Date\":\"01-01-2024\", \"Jour\":\"Lundi\", \"Mois\":\"Janvier\"},\n",
    "    {\"Nom\":\"Andriamisa\", \"Prenom\":\"Dylan\"}\n",
    "]\n",
    "collection.insert_many(data1)\n",
    "\n",
    "\n",
    "# Consultation\n",
    "for doc in collection.find() :\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression\n",
    "collection.delete_one({\"Nom\":\"Andriamisa\"})\n",
    "collection.delete_many({\"Mois\":\"Decembre\"})\n",
    "\n",
    "\n",
    "# Consultation\n",
    "for doc in collection.find() :\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification\n",
    "collection.update_one({\"Date\":\"01-01-2024\"},{\"$set\":{\"Date\":\"23-12-2024\",\"Mois\":\"Decembre\"}})\n",
    "collection.update_many({\"Nom\":\"Andriamisa\"},{\"$set\":{\"Statut\":\"Etudiant\"}})\n",
    "\n",
    "\n",
    "# Consultation\n",
    "for doc in collection.find() :\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Les perfomances avec indexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_bl_mongo(csv_file, n):\n",
    "    db = connection[\"db_R5A10\"]\n",
    "    bloom_collection = db['bloom']\n",
    "\n",
    "    bloom_collection.drop()\n",
    "\n",
    "\n",
    "    bloom_collection.create_index('codecommune', unique=True)\n",
    "\n",
    "    with open(csv_file, encoding='utf-8') as csvfile:\n",
    "        my_reader = csv.DictReader(csvfile, delimiter=',')\n",
    "        my_data = [my_row for my_row in my_reader]\n",
    "\n",
    "        pres = dup = 0\n",
    "        print('Début de la vérification des doublons sur', n, 'entrées')\n",
    "        st = time.process_time()\n",
    "\n",
    "        for my_row in my_data[:n]:\n",
    "\n",
    "            try:\n",
    "                bloom_collection.insert_one({'codecommune': my_row['codecommune']})\n",
    "                pres += 1\n",
    "            except Exception as e:\n",
    "\n",
    "                dup += 1\n",
    "\n",
    "        et = time.process_time()\n",
    "        res = et - st\n",
    "        print('Temps d\\'exécution CPU :', res, 'secondes')\n",
    "        print('Trouvé', dup, 'doublons dans l\\'entrée')\n",
    "\n",
    "perf_bl_mongo(\"pres2022comm.csv\", 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "Dans le cadre de cette ressource sur les nouvelles bases de données (R5.A.10), nous avons exploré différentes facettes des bases de données, notamment la dénormalisation d'un schéma relationnel et l'étude de deux systèmes de gestion de bases de données (SGBD) NoSQL, à savoir Redis et MongoDB.\n",
    "\n",
    "### **Dénormalisation**\n",
    "La dénormalisation a été abordée comme une technique visant à fusionner des tables SQL en une seule entité, réduisant ainsi le besoin de jointures fréquentes. Les critères à respecter pour la dénormalisation ont été présentés, et un exemple concret de dénormalisation d'un schéma relationnel SQL a été illustré.\n",
    "\n",
    "### **Redis**\n",
    "Redis, en tant que SGBD NoSQL, a été exploré dans divers aspects. Nous avons couvert l'utilisation des filtres de Bloom sur Redis pour tester l'appartenance d'un élément à un ensemble. Les performances de Redis, en particulier avec l'utilisation de Bloom Filters et l'ensemble SET_PY, ont été évaluées. De plus, nous avons exploré le modèle de publication/abonnement (PUB/SUB) avec un exemple concret utilisant des messages CO2 générés aléatoirement.\n",
    "\n",
    "### **MongoDB**\n",
    "MongoDB, un autre SGBD NoSQL, a été présenté en détail. Nous avons effectué des opérations de base telles que l'ajout, la consultation, la suppression et la modification de données dans MongoDB. Les performances de MongoDB ont été évaluées en utilisant des index pour améliorer la recherche des doublons dans un ensemble de données.\n",
    "\n",
    "### **Performances Comparées**\n",
    "En termes de performances, Redis s'est avéré légèrement plus rapide dans l'utilisation de la structure de données Bloom Filters, avec une moyenne de 0.8 seconde par rapport à MongoDB qui a montré une moyenne de 1.2 secondes pour 2000 entrées sur le même fichier \"DEMO.csv\".\n",
    "\n",
    "### **Conclusion Générale**\n",
    "Le choix entre Redis et MongoDB dépend des besoins spécifiques d'utilisation et du modèle de données. Redis est souvent privilégié pour des opérations rapides en mémoire, tandis que MongoDB peut être plus adapté pour des applications nécessitant des opérations de recherche complexes sur des ensembles de données volumineux."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

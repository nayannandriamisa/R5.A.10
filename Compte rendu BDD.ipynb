{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compte rendu R5.A.10, ANDRIAMISA Nayann, Groupe Nayru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce fichier, nous verrons un compte rendu de la ressource du BUT informatique (R5.A.10) portant sur les nouvelles bases de données.    \n",
    "Elle est découpée en 3 grandes parties qui sont respectivement, sur la dénormalisation d'un schéma relationnel de tables et puis d'études des Systèmes de Gestion de Base de Données (SGBD) **Redis** et **MongoDB**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire :\n",
    "\n",
    "<ol>\n",
    "    <li><strong>Dénormalisation</strong></li>\n",
    "    <ol>\n",
    "        <li>Dénormalisation d'un schéma relationnel </li>\n",
    "        <li>Application de la représentation JSON</li>\n",
    "    </ol>\n",
    "    <li><strong>Redis</strong></li>\n",
    "    <ol>\n",
    "        <li>Bloom filters </li>\n",
    "        <li>SET_PY</li>\n",
    "        <li>PUB/SUB</li>\n",
    "    </ol>\n",
    "    <li><strong>MongoDB</strong></li>\n",
    "    <ol>\n",
    "        <li>Premiers pas avec MongoDB</li>\n",
    "        <li>Performances avec indexe</li>\n",
    "        <li>Performances sans indexe</li>\n",
    "    </ol>\n",
    "</ol>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dénormalisation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit de faire une sorte de fusion/factorisation d'un ensemble de tables SQL en une seule pour avoir une \"structure à plat\" afin de minimiser les jointures nécessaires aux opérations.    \n",
    "Le problème qui se pose est donc de savoir comment passer d'un schéma relationnel en SQL à un modèle NoSQL qui pourrait être implémenté avec nos gestionnaires.    \n",
    "<br>\n",
    "Il y a certains critères à respecter dans la fusion dans le cas d'une dénormalisation : \n",
    "- Les données qui sont fréquemment intérrogées doivent être rassemblées\n",
    "- Toutes les données d'un entité doivent être indépendantes\n",
    "- Une association avec des relations 1+n des deux côtés\n",
    "- Le taux de mise à jour est le même\n",
    "\n",
    "Une fois un schéma relationnel dénormalisé, pour être utilisé il convient de l'obetnir sous une représentation JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Exemple/exercice :</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dénormalisation d'un schéma relationnel SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un schéma relationnel en SQL, qui a déjà été étudié par les étudiants de 1ère année de BUT informatique pour la ressource ***Introduction aux bases de données et SQL*** :\n",
    "<br><br>\n",
    "<code>\n",
    "**AVIONS** (*NumAv, NomAv, CapAv, VilleAv*);<br>\n",
    "**PILOTES** (*NumPil, NomPil, NaisPil, VillePil*);<br>\n",
    "**CLIENTS**(*NumCl, NomCl, NumRueCl, NomRueCl, CodePosteCl, VilleCl*);<br>\n",
    "**VOLS**(*NumVol, VilleD, VilleA, DateD, HD, DateA, HA, NumPil, NumAv*);<br>\n",
    "**DEFCLASSES**(*NumVol, Classe, CoeffPrix*);<br>\n",
    "**RESERVATIONS** (*NumCl, NumVol, Classe, NbPlaces*);<br>\n",
    "</code>\n",
    "<br>\n",
    "Et afin de procéder à une dénormalisation de ce schéma relationnel, nous avons fusionné les tables **VOLS**, **DEFCLASSES** et **RESERVATIONS** par rapport à l'attribut \"*NumVol*\".    \n",
    "Voici le résultat obtenu après dénormalisation :\n",
    "\n",
    "<code>\n",
    "**AVIONS** (*NumAv, NomAv, CapAv, VilleAv*);<br>\n",
    "**PILOTES** (*NumPil, NomPil, NaisPil, VillePil*);<br>\n",
    "**CLIENTS**(*NumCl, NomCl, NumRueCl, NomRueCl, CodePosteCl, VilleCl*);<br>\n",
    "**VOLS**(*NumVol, VilleD, VilleA, DateD, HD, DateA, HA, NumPil, NumAv, Classe, CoeffPrix, NumCl, NbPlaces*)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Application de la représentation JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc commencer par mettre les fichiers dans le bon format, et devoir passer d'un document .txt à un document JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importations et installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On va commencer par les convertir en fichier .csv\n",
    "En utilisant la fonction <code>to_csv()</code> de la libraire Python : **Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avions = pd.read_csv('bddPilotes/AVIONS.txt', sep='\\t')\n",
    "avions.columns = [\"NumAv\", \"NomAv\", \"CapAv\", \"VilleAv\"]\n",
    "avions.to_csv('bddPilotes/CSV/AVIONS.csv', index=None)\n",
    "\n",
    "pilotes = pd.read_csv('bddPilotes/PILOTES.txt', sep='\\t')\n",
    "pilotes.columns = [\"NumPil\",\"NomPil\",\"NaisPil\",\"VillePil\"]\n",
    "pilotes.to_csv('bddPilotes/CSV/PILOTES.csv', index=None)\n",
    "\n",
    "clients = pd.read_csv('bddPilotes/CLIENTS.txt', sep='\\t')\n",
    "clients.columns = [\"NumCl\", \"NomCl\", \"NumRueCl\", \"NomRueCl\", \"CodePosteCl\", \"VilleCl\"]\n",
    "clients.to_csv('bddPilotes/CSV/CLIENTS.csv', index=None)\n",
    "\n",
    "vols = pd.read_csv('bddPilotes/VOLS.txt', sep='\\t')\n",
    "vols.columns = [\"NumVol\",\"VilleD\",\"VilleA\", \"DateD\",\"HD\",\"DateA\",\"HA\", \"NumPil\", \"NumAv\"]\n",
    "vols.to_csv('bddPilotes/CSV/VOLS.csv', index=None)\n",
    "\n",
    "defclasses = pd.read_csv('bddPilotes/DEFCLASSES.txt', sep='\\t')\n",
    "defclasses.columns = [\"NumVol\",\"Classe\", \"CoeffPrix\"]\n",
    "defclasses.to_csv('bddPilotes/CSV/DEFCLASSES.csv', index=None)\n",
    "\n",
    "reservations = pd.read_csv('bddPilotes/RESERVATIONS.txt', sep='\\t')\n",
    "reservations.columns = [\"NumCl\", \"NumVol\" ,\"Classe\", \"NbPlaces\"]\n",
    "reservations.to_csv('bddPilotes/CSV/RESERVATIONS.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis nous allons effectuer la jointure sur les classes **VOLS**, **DEFCLASSES** et **RESERVATIONS** en utilisant la fonction <code>merge()</code> pour l'étape de la dénormalisation.    \n",
    "Et nous finirons par convertir ces fichiers .csv au format JSON grâce à la fonction <code>to_json()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avions = pd.read_csv('bddPilotes/CSV/AVIONS.csv')\n",
    "pilotes = pd.read_csv('bddPilotes/CSV/PILOTES.csv')\n",
    "clients = pd.read_csv('bddPilotes/CSV/CLIENTS.csv')\n",
    "vols = pd.read_csv('bddPilotes/CSV/VOLS.csv')\n",
    "defclasses = pd.read_csv('bddPilotes/CSV/DEFCLASSES.csv')\n",
    "reservations = pd.read_csv('bddPilotes/CSV/RESERVATIONS.csv')\n",
    "\n",
    "fusion_vols_defclasses = pd.merge(vols, defclasses, how='inner', on=[\"NumVol\"])\n",
    "fusion_vols_defclasses_reservations = pd.merge(fusion_vols_defclasses, reservations, how='inner', on=[\"NumVol\"])\n",
    "fusion_vols_defclasses_reservations.to_csv('bddPilotes/CSV/VolsClassesReservations.csv', index=None)\n",
    "\n",
    "#print(fusion_vols_defclasses_reservations.head(50))\n",
    "\n",
    "avions.to_json('bddPilotes/JSON/AVIONS.json', orient=\"records\")\n",
    "pilotes.to_json('bddPilotes/JSON/PILOTES.json', orient=\"records\")\n",
    "clients.to_json('bddPilotes/JSON/CLIENTS.json', orient=\"records\")\n",
    "vols.to_json('bddPilotes/JSON/VOLS.json', orient=\"records\")\n",
    "defclasses.to_json('bddPilotes/JSON/DEFCLASSES.json', orient=\"records\")\n",
    "reservations.to_json('bddPilotes/JSON/RESERVATIONS.json', orient=\"records\")\n",
    "\n",
    "fusion = pd.read_csv('bddPilotes/CSV/VolsClassesReservations.csv')\n",
    "fusion.to_json('bddPilotes/JSON/VolsClassesReservations.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bddPilotes/JSON/AVIONS.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    print(data)\n",
    "\n",
    "with open('bddPilotes/JSON/PILOTES.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    print(data)\n",
    "\n",
    "with open('bddPilotes/JSON/VolsClassesReservations.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fonction de jointure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jointure(fic1, fic2, attribut) :\n",
    "    mapping_data1 = {item[colonne_jointure]: item for item in data1}\n",
    "\n",
    "    # Fusionner les données en fonction de la colonne de jointure\n",
    "    resultat = []\n",
    "    for item2 in data2:\n",
    "        item1 = mapping_data1.get(item2[colonne_jointure], {})\n",
    "        resultat.append({**item1, **item2})\n",
    "\n",
    "    return resultat\n",
    "\n",
    "'''\n",
    "def jointure(json1, json2):\n",
    "\n",
    "    from json import loads\n",
    "    from json import dumps\n",
    "\n",
    "    # First, transform json objects to dictionaries\n",
    "\n",
    "    d1_name = list(loads(json1))[0]\n",
    "    #print(d1_name)\n",
    "    d2_name = list(loads(json2))[0]\n",
    "    #print(d2_name)\n",
    "\n",
    "    d1 = loads(json1)[d1_name]\n",
    "    d2 = loads(json2)[d2_name]\n",
    "\n",
    "    #print(att_name,type(att_name))\n",
    "    # Second, iterate through dictionaries\n",
    "    d_res = {}\n",
    "    for key1, val1 in d1.items():\n",
    "        #print(key1, '==', val1)\n",
    "        for key2, val2 in d2.items():\n",
    "            #print(key1, '==', key2)\n",
    "            #print([ord(c) for c in key1],key1,[ord(c) for c in att_name],att_name)\n",
    "            if key1 == key2:\n",
    "                d = {}\n",
    "                d.update(val1)\n",
    "                d.update(val2)\n",
    "                #print(d)\n",
    "                d_res[key1] = d\n",
    "    my_my_dict = {}\n",
    "    my_my_dict['test'] = d_res\n",
    "    z = dumps(my_my_dict)\n",
    "\n",
    "    return z\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redis : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis est un SGBD et aussi un gestionnaire **NoSQL** *(Not Only SQL)* qui permet d'adopter le modèle \"clé-valeur\", ou aussi **JSON**, pour sa structure de données.    \n",
    "<br>\n",
    "Afin de se connecter à un serveur Redis depuis ce fichier Jupyter Notebook, pour pouvoir expérimenter ce gestionnaire, j'ai fait appel à ***Docker*** en lancant mon serveur depuis un conteneur avec la commande <code>redis-cli</code>    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Connexion à Redis</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Les installations et importations nécessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import csv\n",
    "from json import dumps\n",
    "import time\n",
    "\n",
    "client = redis.Redis(host='localhost', port=6380, decode_responses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test de la connection au serveur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout de données\n",
    "client.set(\"Nom\", \"Andriamisa\")\n",
    "client.set(\"Prenom\", \"Nayann\")\n",
    "\n",
    "# Récupération des données\n",
    "print(client.get(\"Nom\") + \" \" + client.get(\"Prenom\"))\n",
    "client.get(\"Ressource\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Les Bloom filters sur Redis</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le filtre de Bloom est une structure de données probabiliste utilisée pour tester l'appartenance d'un élément à un ensemble. Conçu pour être utilisé dans des situations où la mémoire est limitée et où des faux positifs sont tolérables, le filtre de Bloom permet d'économiser de l'espace en évitant le stockage direct des éléments de l'ensemble. Il utilise plusieurs fonctions de hachage pour attribuer à chaque élément plusieurs positions dans un tableau de bits. Lorsqu'on interroge le filtre pour la présence d'un élément, il renvoie probablement vrai (avec une certaine probabilité d'erreur), indiquant ainsi que l'élément pourrait être dans l'ensemble, ou certainement faux s'il n'est pas présent. Redis offre une implémentation efficace du filtre de Bloom, permettant son utilisation dans un contexte de base de données à mémoire vive avec des avantages significatifs en termes d'optimisation de l'utilisation de la mémoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Le code python implémentant les Blooms filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_bloom(csv_file, n) :\n",
    "\n",
    "    client.delete(\"bloom\")\n",
    "    client.execute_command(\"BF.RESERVE\", \"bloom\", \"0.01\", \"1700000\")\n",
    "\n",
    "    with open(csv_file, encoding='utf-8') as csvfile:\n",
    "        my_reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "        my_data = [my_row for my_row in my_reader]\n",
    "\n",
    "        pres = dup = 0\n",
    "        print('Start to create the bloom filter over', n, 'inputs')\n",
    "        st = time.process_time()\n",
    "\n",
    "        for my_row in my_data[0:n]:\n",
    "            if not client.execute_command(\"BF.EXISTS\", \"bloom\", my_row['M']):\n",
    "                client.execute_command(\"BF.ADD\", \"bloom\", my_row['M'])\n",
    "                pres = pres + 1\n",
    "            else:\n",
    "                dup = dup + 1\n",
    "\n",
    "        et = time.process_time()\n",
    "        res = et - st\n",
    "        print('CPU Execution time:', res, 'seconds')\n",
    "        print('We found', dup, 'duplicates in the input')\n",
    "\n",
    "# Step 1\n",
    "perf_bloom(\"DEMO.csv\", 14000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Set.py pour des test de performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici une fonction qui teste les performances d'ajout d'éléments distincts du fichier CSV à un ensemble Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_set(csv_file, n):\n",
    "\n",
    "    r = redis.Redis(host='localhost', port=6380, decode_responses=True)\n",
    "\n",
    "    r.delete(\"my_set\")\n",
    "\n",
    "    with open(csv_file, encoding = 'utf-8') as csvfile:\n",
    "        my_reader = csv.DictReader(csvfile,delimiter='\\t')\n",
    "        my_data = [my_row for my_row in my_reader]\n",
    "        #print(my_data)\n",
    "        pres = 0\n",
    "        dup = 0\n",
    "        print('Start to create the set over',n,'inputs')\n",
    "        # get the start time\n",
    "        st = time.process_time()\n",
    "        for my_row in my_data[0:n]:\n",
    "            #print(my_row['M'])\n",
    "            if not r.sismember(\"my_set\", my_row['M']):\n",
    "                r.sadd(\"my_set\", my_row['M'])\n",
    "                pres += 1\n",
    "            else:\n",
    "                dup += 1\n",
    "        # get the end time\n",
    "        et = time.process_time()\n",
    "        # get execution time\n",
    "        res = et - st\n",
    "        print('CPU Execution time:', res, 'seconds')\n",
    "        print('We found',dup,'duplicates in the input')\n",
    "        print()\n",
    "        print('Wall time (also known as clock time or wall-clock time) is simply the total time')\n",
    "        print('elapsed during the measurement. It\\'s the time you can measure with a stopwatch.')\n",
    "        print('It is the difference between the time at which a program finished its execution and')\n",
    "        print('the time at which the program started. It also includes waiting time for resources.')\n",
    "        print()\n",
    "        print('CPU Time, on the other hand, refers to the time the CPU was busy processing')\n",
    "        print('the program\\'s instructions. The time spent waiting for other task to complete')\n",
    "        print('(like I/O operations) is not included in the CPU time. It does not include')\n",
    "        print('the waiting time for resources.')\n",
    "#Step 1\n",
    " \n",
    "perf_set(\"DEMO.csv\", 1400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUB/SUB avec Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les installations et importations utilisées pour tester PUB/SUB avec Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import schedule\n",
    "\n",
    "client = redis.Redis(host=\"localhost\", port=6380, decode_responses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le code ci-dessous, la fonction <code>publish()</code> va publier un message qui sera un couple d'une **date** (à l'occurence la date du jour ainsi que l'heure) et d'une **CO2_value** (un nombre aléatoire entre 300 et 1000).    \n",
    "Ces messages seront publiés de manière espacée entre 1 et 3 secondes (aléatoirement aussi).  \n",
    "<br>  \n",
    "*Afin que la fonction PUB/SUB de Redis fonctionne correctemment, il faut éxécuter le publisher indépendamment du subscriber (dans un autre jupyter avec un noyau différent de celui utilisé dans ce compte rendu par exemple).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_co2_value():\n",
    "    return round(random.uniform(300, 1000))\n",
    "\n",
    "while True:\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    co2_value = generate_random_co2_value()\n",
    "    \n",
    "    # Créer un dictionnaire\n",
    "    data = {'date': current_date, 'co2_value': co2_value}\n",
    "    \n",
    "    # Convertir le dictionnaire en format JSON\n",
    "    json_data = json.dumps(data)\n",
    "    \n",
    "    client.publish('testPubSub', json_data)\n",
    "\n",
    "    #print(r.get('testPubSub'))\n",
    "\n",
    "    time.sleep(random.uniform(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subscriber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le code ci-dessous, la fonction <code>pubsub()</code> va définir le comportement du client et la fonction <code>psubscribe()</code> va dire que le client connecté est \"abonné\" aux messages publiés dans la chaîne \"testPubSub\".    \n",
    "Et elle executera toutes les minutes un calcul de la moyenne des valeurs de CO2 que le client a reçu dans la dernière minute.\n",
    "<br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "    global my_list\n",
    "    #\n",
    "    # Calcul de la moyenne\n",
    "    #\n",
    "    if len(my_list) > 0:\n",
    "    \tprint(\"La moyenne des valeurs de CO2 sur la dernière minute est:\", sum(my_list) / len(my_list))\n",
    "    my_list.clear()\n",
    "\n",
    "schedule.every(10).seconds.do(func)\n",
    "\n",
    "p = client.pubsub()\n",
    "p.psubscribe('testPubSub')\n",
    "\n",
    "my_list = []\n",
    "\n",
    "first_message = True\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    m = p.get_message()\n",
    "    #schedule.run_pending()\n",
    "    if m != None:\n",
    "       if not first_message:\n",
    "          print('Message reçu:', m['data'])\n",
    "          data_str = m['data']\n",
    "          #data_str = m['data'].decode('utf-8')  # Décoder les bytes en str\n",
    "          data_dict = json.loads(data_str)  # Charger la chaîne JSON en un dictionnaire Python\n",
    "          co2_value = data_dict.get('co2_value')\n",
    "          if co2_value is not None:\n",
    "                my_list.append(co2_value)\n",
    "\n",
    "       else:\n",
    "        print(\"Erreur : ça marche pas\")\n",
    "        first_message = False\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB est un SGBD et aussi un gestionnaire **NoSQL** *(Not Only SQL)* qui permet d'adopter le modèle  **JSON** pour sa structure de données.  \n",
    "<br>\n",
    "Afin de se connecter à un serveur MongoDB depuis ce fichier Jupyter Notebook, pour pouvoir expérimenter ce gestionnaire, j'ai fait appel à ***Docker*** en lancant mon serveur depuis un conteneur avec la commande <code>mongosh</code>    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les installations et importations nécessaires pour l'expérimentation de MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "connection = pymongo.MongoClient(host=\"localhost\", port=27017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Premiers pas avec MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = connection[\"db_R5A10\"]\n",
    "\n",
    "collection = db[\"test_bd_jupyter\"]\n",
    "\n",
    "# Ajout\n",
    "data = {\"nom\":\"Andriamisa\", \"Prenom\":\"Nayann\"}\n",
    "collection.insert_one(data)\n",
    "data = {\"Date\":\"22-12-2023\", \"Jour\":\"Vendredi\"}\n",
    "collection.insert_one(data)\n",
    "\n",
    "\n",
    "# Modification\n",
    "collection.update_one({\"Date\":\"22-12-2023\"},{\"$set\":{\"Date\":\"23-12-2023\"}})\n",
    "\n",
    "# Suppression\n",
    "collection.delete_many({\"Date\":\"23-12-2023\"})\n",
    "\n",
    "# Consultation\n",
    "for doc in collection.find() :\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
